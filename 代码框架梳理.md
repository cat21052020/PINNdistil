# Ψ-NN 代码框架梳理文档

## 项目概述

**Ψ-NN** 是一个基于知识蒸馏的物理信息神经网络（PINN）项目，采用"**蒸馏-结构提取-网络重构**"三步法，自动发现并隐式嵌入PDE的物理特性到MLP网络结构中。

**论文信息**：Nature Communications (2025), DOI: 10.1038/s41467-025-64624-3

---

## 一、整体架构

### 1.1 核心流程

```
配置加载 → 模型初始化 → 教师网络训练 → 知识蒸馏 → 学生网络训练 → 结构提取 → 结果可视化
```

### 1.2 目录结构

```
Psi-NN-main/
├── Panel.py                    # 主入口文件，任务调度
├── Config/                     # 配置文件目录（CSV格式）
│   ├── Burgers_inv_distill_EXP.csv    # 知识蒸馏实验配置
│   ├── Burgers_inv_EXP.csv            # 对比实验配置
│   └── ...
├── Database/                   # 数据目录
│   ├── Burgers_inv_data_*.csv  # 逆问题监督数据
│   └── flow/                   # 流场问题数据
├── Module/                     # 核心模块目录
│   ├── Training.py             # 核心训练流程（最重要）
│   ├── PINN.py                 # 基础PINN网络结构
│   ├── PsiNN_*.py              # 不同问题的专用网络结构
│   ├── PINN_post_*.py          # 不同硬映射函数的PINN变体
│   ├── SingleVis.py            # 单模型可视化
│   └── GroupVis.py             # 多模型对比可视化
├── Results/                     # 结果输出目录（自动生成）
└── pic_Parameter.ipynb         # 参数聚类可视化工具
```

---

## 二、核心模块详解

### 2.1 入口文件：`Panel.py`

**功能**：任务调度器，串行执行多个训练任务

**关键代码**：
```python
task_5 = Training.model('Burgers_inv_distill', 'EXP')  # 知识蒸馏任务
task_5.train()
```

---

### 2.2 核心训练模块：`Training.py`

这是整个项目的**核心**，包含完整的训练流程。

#### 2.2.1 `model` 类初始化 (`__init__`)

**功能**：从CSV配置文件加载所有超参数

**关键参数**：
- `ques_name`: 问题名称（如 'Burgers_inv_distill'）
- `distill_state`: 是否启用知识蒸馏（从文件名自动判断）
- `model`: 教师网络模型列表
- `hidden_layers_group`: 教师网络隐藏层神经元组数
- `hidden_layers_group_student`: 学生网络隐藏层神经元组数
- `k_value`: 蒸馏学习系数（控制教师输出与观测值的权重）
- `train_ratio`: 学生网络训练步数比例（相对于教师网络）

#### 2.2.2 网格初始化 (`mesh_init`)

**功能**：根据问题类型初始化计算场网格点

**支持的问题类型**：
- 2D问题：标准网格
- 3D问题：三维网格
- Flow问题：从CSV文件读取流体数据点

#### 2.2.3 损失函数定义

##### (1) `net_f()` - 控制方程损失

**功能**：计算PDE残差损失

**支持的方程**：
- **Burgers方程**：`u_t + u*u_x - λ*u_xx = 0`
- **Laplace方程**：`u_xx + u_yy = 0`
- **Poisson方程**：`u_xx + u_yy = f(x,y)`
- **Navier-Stokes方程**：连续性方程 + 动量方程

##### (2) `net_b()` - 边界条件损失

**功能**：计算边界条件约束损失

**边界类型**：
- 固定值边界
- 周期性边界
- 流场问题的入口/出口/壁面边界

##### (3) `net_d()` - 数据监督损失

**功能**：计算观测数据与网络输出的MSE损失

**知识蒸馏特殊处理**（`mode='student'`）：
- 当 `k_value > 0` 时，使用自适应权重融合教师输出和观测值
- 公式：`loss_d = mean((1-fai) * (student_output - u_monitor)^2)`
- 其中 `fai = 1 - tanh(k_value * |teacher_output - u_monitor|)`

##### (4) `net_teach()` - 知识蒸馏损失

**功能**：计算学生网络与教师网络的输出差异

**公式**：`loss_teach = mean((u_teacher - u_student)^2) * weight_teach`

**特殊处理**：
- 当 `k_value > 0` 时，排除已有观测值的坐标点，避免重复学习

##### (5) `net_rgl()` - 正则化损失

**功能**：L1/L2/GrOWL正则化

**学生网络正则化**：
- 仅对权重进行正则化（`object='weight'`）
- 可通过 `study_regularization_state` 控制是否启用

#### 2.2.4 训练流程 (`train_adam`)

**核心训练循环**：

```python
for iter_group in range(step_num):  # 训练组数
    # ========== 教师网络训练阶段 ==========
    for iter_inner in range(train_steps):
        # 1. 计算损失
        loss_f = net_f()              # 控制方程损失
        loss_d = net_d()               # 数据损失（如果有）
        loss_b = net_b()               # 边界损失
        loss_rgl = net_rgl()           # 正则化损失
        
        # 2. 总损失
        loss = loss_f + loss_d + loss_b + loss_rgl
        
        # 3. 反向传播和优化
        loss.backward()
        optimizer.step()
    
    # ========== 知识蒸馏阶段（如果启用）==========
    if distill_state:
        for iter_inner in range(int(train_steps * train_ratio)):
            # 1. 学生网络损失
            loss_student_d = net_d(mode='student')    # 数据损失（含自适应权重）
            loss_teach = net_teach()                  # 知识蒸馏损失
            loss_student_rgl = net_rgl(mode='student') # 正则化损失
            
            # 2. 总损失
            loss_student = loss_student_d + loss_teach + loss_student_rgl
            
            # 3. 反向传播和优化
            loss_student.backward()
            optimizer_student.step()
```

**关键特性**：
- **交替训练**：先训练教师网络一个阶段，再训练学生网络
- **自适应权重**：通过 `k_value` 控制教师输出与观测值的融合比例
- **参数记录**：实时记录所有损失值和参数变化

#### 2.2.5 模型保存 (`model_save`)

**保存内容**：
- 模型权重（`.pth`文件）
- 损失曲线数据（`.csv`文件）
- 参数演化数据（逆问题时）
- 训练时间记录

---

### 2.3 网络结构模块

#### 2.3.1 `PINN.py` - 基础PINN网络

**结构**：标准MLP（多层感知机）

```python
class Net(torch.nn.Module):
    def __init__(self, layers):
        # layers: [input_dim, hidden1, hidden2, ..., output_dim]
        # 使用Tanh激活函数
```

**用途**：
- 教师网络的基础结构
- 学生网络的标准结构

#### 2.3.2 `PsiNN_*.py` - 专用网络结构

**特点**：针对特定PDE问题设计的特殊网络结构

**示例：`PsiNN_burgers.py`**
- 第一层：分离x和y坐标，使用不同的线性变换
- 第二层：组合第一层的输出，形成对称结构
- 第三层：进一步组合，提取特征
- 第四层：输出层

**设计理念**：通过特殊的网络结构隐式嵌入PDE的对称性和物理特性

---

### 2.4 可视化模块

#### 2.4.1 `SingleVis.py`

**功能**：
- 2D/3D场可视化
- 损失曲线绘制
- 参数演化曲线（逆问题时）

#### 2.4.2 `GroupVis.py`

**功能**：多模型对比可视化
- 损失曲线对比
- 参数演化对比

#### 2.4.3 `pic_Parameter.ipynb`

**功能**：参数聚类分析

**步骤**：
1. 加载训练好的学生网络权重
2. 对每层权重进行层次聚类
3. 识别聚类中心
4. 用字母代号表示聚类模式
5. 可视化权重分布和聚类结果

**用途**：发现网络结构中的对称性和重复模式

---

## 三、知识蒸馏机制详解

### 3.1 蒸馏流程

```
┌─────────────────┐
│  教师网络训练    │ ← 标准PINN训练（控制方程+边界+数据）
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  知识蒸馏阶段    │
│                 │
│  loss_student = │
│    loss_d +     │ ← 数据损失（含自适应权重）
│    loss_teach + │ ← 教师-学生输出差异
│    loss_rgl     │ ← 权重正则化
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  学生网络输出    │
└─────────────────┘
```

### 3.2 自适应权重机制（`k_value > 0`）

**核心思想**：在观测值附近，学生网络主要学习观测值；远离观测值的地方，学习教师网络的输出。

**数学表达**：
```
fai = 1 - tanh(k_value * |teacher_output - u_monitor|)

loss_d = mean((1-fai) * (student_output - u_monitor)^2)
```

**效果**：
- `k_value` 越大，自适应效果越明显
- 在观测点附近：`fai ≈ 0`，主要学习观测值
- 远离观测点：`fai ≈ 1`，主要学习教师输出

### 3.3 结构提取

**方法**：通过权重聚类发现网络结构模式

**步骤**：
1. 训练完成后，对学生网络权重进行层次聚类
2. 识别聚类中心（代表不同的权重值）
3. 用字母代号表示聚类模式（如 A, B, -A, -B）
4. 分析对称性和重复结构

**意义**：揭示网络如何隐式学习PDE的物理结构

---

## 四、配置文件格式

### 4.1 关键配置项

| 配置项 | 说明 | 示例 |
|--------|------|------|
| `model` | 教师网络名称（空格分隔多个） | `PINN PINN_post_minus PsiNN_burgers` |
| `hidden_layers_group` | 教师网络隐藏层神经元组数 | `1,1,2` |
| `hidden_layers_group_student` | 学生网络隐藏层神经元组数 | `1,1,1` |
| `k_value` | 蒸馏学习系数 | `0`（标准蒸馏）或 `>0`（自适应） |
| `train_ratio` | 学生网络训练步数比例 | `2`（学生训练步数是教师的2倍） |
| `step_num` | 训练组数 | `1` |
| `train_steps` | 单组训练步数 | `100000` |

### 4.2 知识蒸馏配置示例

**`Burgers_inv_distill_EXP.csv`**：
```csv
model,PINN
hidden_layers_group,"1,1,2"
hidden_layers_group_student,"1,1,1"
k_value,0
train_ratio,2
```

---

## 五、使用流程

### 5.1 标准PINN训练

```python
task = Training.model('Burgers_inv', 'EXP')
task.train()
```

### 5.2 知识蒸馏训练

```python
task = Training.model('Burgers_inv_distill', 'EXP')
task.train()
```

### 5.3 结构提取分析

1. 运行训练，生成模型文件
2. 打开 `pic_Parameter.ipynb`
3. 修改配置：
   ```python
   ques_name = 'Burgers_inv_distill'
   mode = 'student'
   file_path = f"./Results/{ques_name}_{ini_num}/Models/..."
   ```
4. 运行notebook，查看聚类结果

---

## 六、关键设计特点

### 6.1 模块化设计

- **配置驱动**：所有超参数通过CSV文件配置
- **问题解耦**：不同PDE问题有独立的网络结构模块
- **可视化分离**：可视化功能独立封装

### 6.2 知识蒸馏创新点

1. **自适应权重融合**：通过 `k_value` 实现观测值与教师输出的智能融合
2. **交替训练**：教师和学生网络交替训练，而非同时训练
3. **结构提取**：通过聚类分析发现网络学习的物理结构

### 6.3 扩展性

- **新问题添加**：只需添加新的 `PsiNN_*.py` 和配置文件
- **新损失函数**：在 `Training.py` 中添加新的损失计算方法
- **新可视化**：扩展 `SingleVis.py` 或 `GroupVis.py`

---

## 七、文件依赖关系

```
Panel.py
  └── Training.model
       ├── PINN.Net (基础网络)
       ├── PsiNN_*.Net (专用网络)
       ├── SingleVis.Vis (可视化)
       └── GroupVis.Vis (对比可视化)
```

---

## 八、关键函数调用链

### 8.1 训练流程

```
train()
  └── workflow()
       ├── mesh_init()          # 初始化网格
       ├── train_adam()         # 训练循环
       │    ├── net_f()         # 控制方程损失
       │    ├── net_b()         # 边界损失
       │    ├── net_d()         # 数据损失
       │    ├── net_teach()     # 蒸馏损失（如果启用）
       │    └── net_rgl()       # 正则化损失
       ├── model_save()         # 保存模型和结果
       └── result_show()        # 可视化结果
```

### 8.2 知识蒸馏流程

```
train_adam()
  ├── [教师网络训练循环]
  │    └── loss = loss_f + loss_d + loss_b + loss_rgl
  │
  └── [学生网络训练循环] (if distill_state)
       ├── loss_student_d = net_d(mode='student')
       ├── loss_teach = net_teach()
       ├── loss_student_rgl = net_rgl(mode='student')
       └── loss_student = loss_student_d + loss_teach + loss_student_rgl
```

---

## 九、常见问题类型

### 9.1 正问题（Forward Problem）

- **Laplace**: 求解Laplace方程
- **Poisson**: 求解Poisson方程
- **Burgers**: 求解Burgers方程
- **Flow**: 求解稳态Navier-Stokes方程

### 9.2 逆问题（Inverse Problem）

- **Burgers_inv**: 已知部分数据，反推Burgers方程参数
- **Laplace_inv**: Laplace方程逆问题
- **Poisson_inv**: Poisson方程逆问题

### 9.3 知识蒸馏问题

- **Burgers_inv_distill**: Burgers逆问题的知识蒸馏版本
- 其他问题的 `*_distill` 变体

---

## 十、输出结果

### 10.1 目录结构

```
Results/
└── {ques_name}_{ini_num}/
    ├── Models/                    # 模型权重文件
    │   ├── {model_name}.pth
    │   └── {model_name}_student_step_{iter}.pth
    ├── Loss/                      # 损失数据
    │   ├── {model_name}_loss.csv
    │   └── {model_name}_loss_student.csv
    ├── Parameters/                # 参数演化（逆问题时）
    │   └── {model_name}_paras.csv
    ├── Figures/                   # 可视化图片
    └── Cluster/                   # 聚类结果（如果运行notebook）
```

### 10.2 关键输出文件

- **模型权重**：`.pth` 文件，包含网络参数
- **损失曲线**：`.csv` 文件，记录训练过程中的所有损失
- **参数演化**：`.csv` 文件（逆问题时），记录参数估计过程
- **可视化图片**：场分布图、损失曲线图等

---

## 十一、总结

### 11.1 核心创新

1. **知识蒸馏 + PINN**：将知识蒸馏引入物理信息神经网络
2. **自适应权重机制**：智能融合观测值和教师输出
3. **结构提取**：通过聚类分析发现网络学习的物理结构

### 11.2 代码特点

- ✅ **模块化**：清晰的模块划分，易于扩展
- ✅ **配置驱动**：所有参数通过CSV配置，无需修改代码
- ✅ **完整流程**：从训练到可视化的完整pipeline
- ✅ **多问题支持**：支持多种PDE问题

### 11.3 适用场景

- 需要压缩PINN模型大小
- 需要从大模型（教师）学习到小模型（学生）
- 需要分析网络学习的物理结构
- 需要处理数据稀疏的逆问题

---

## 附录：关键代码位置速查

| 功能 | 文件 | 函数/类 |
|------|------|---------|
| 主入口 | `Panel.py` | - |
| 训练流程 | `Training.py` | `model.train()` |
| 教师网络训练 | `Training.py` | `train_adam()` (前半部分) |
| 学生网络训练 | `Training.py` | `train_adam()` (后半部分，`distill_state=True`) |
| 知识蒸馏损失 | `Training.py` | `net_teach()` |
| 自适应权重 | `Training.py` | `net_d(mode='student')` (k_value>0时) |
| 基础网络结构 | `PINN.py` | `Net` |
| 专用网络结构 | `PsiNN_*.py` | `Net` |
| 参数聚类分析 | `pic_Parameter.ipynb` | - |

---

**文档版本**：v1.0  
**最后更新**：2025年

